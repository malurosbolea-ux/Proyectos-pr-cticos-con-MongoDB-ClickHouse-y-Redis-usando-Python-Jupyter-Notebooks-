{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrÃ¡ctica 1: IntroducciÃ³n a las bases de datos columnares con ClickHouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clickhouse-connect in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (0.9.2)\n",
      "Requirement already satisfied: faker in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (38.0.0)\n",
      "Requirement already satisfied: certifi in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (from clickhouse-connect) (2025.10.5)\n",
      "Requirement already satisfied: urllib3>=1.26 in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (from clickhouse-connect) (2.3.0)\n",
      "Requirement already satisfied: pytz in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (from clickhouse-connect) (2024.1)\n",
      "Requirement already satisfied: zstandard in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (from clickhouse-connect) (0.23.0)\n",
      "Requirement already satisfied: lz4 in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (from clickhouse-connect) (4.3.2)\n",
      "Requirement already satisfied: tzdata in /Users/maluros/Desktop/anaconda3/lib/python3.13/site-packages (from faker) (2025.2)\n",
      "Cargando config.json y conectando a ClickHouse...\n",
      "---------------------------------\n",
      "Â¡CONEXIÃ“N EXITOSA con p36o6o9jgv.eu-central-1.aws.clickhouse.cloud!\n",
      "El objeto 'client' estÃ¡ listo para usarse en todo el notebook.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# CELDA MAESTRA: InstalaciÃ³n, Imports y ConexiÃ³n\n",
    "# ---------------------------------\n",
    "\n",
    "# 1. Instalamos las librerÃ­as necesarias\n",
    "!pip install clickhouse-connect faker\n",
    "\n",
    "# 2. Imports originales\n",
    "from datetime import date\n",
    "import random\n",
    "from random import randint, choice\n",
    "import time\n",
    "import faker\n",
    "from datetime import datetime\n",
    "import os\n",
    "import clickhouse_connect\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 3. CÃ³digo de conexiÃ³n (para tener el objeto 'client' listo)\n",
    "print(\"Cargando config.json y conectando a ClickHouse...\")\n",
    "try:\n",
    "    # Cargamos las credenciales\n",
    "    with open('config.json') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Creamos el cliente de conexiÃ³n\n",
    "    client = clickhouse_connect.get_client(\n",
    "        host=config['host'],\n",
    "        port=config['port'],\n",
    "        user=config['username'],\n",
    "        password=config['password'],\n",
    "        secure=config['secure']\n",
    "    )\n",
    "\n",
    "    # Hacemos una consulta de prueba rÃ¡pida\n",
    "    client.query(\"SELECT 1\")\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Â¡CONEXIÃ“N EXITOSA con {config['host']}!\")\n",
    "    print(\"El objeto 'client' estÃ¡ listo para usarse en todo el notebook.\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR AL CONECTAR: {e}\")\n",
    "    print(\"AsegÃºrate de que 'config.json' estÃ¡ bien escrito (con la celda del 'Â¡TOMA!')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json cargado. Intentando conectar a TU host: p36o6o9jgv.eu-central-1.aws.clickhouse.cloud...\n",
      "---------------------------------\n",
      "Â¡Â¡Â¡CONEXIÃ“N EXITOSA!!! ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
      "Resultado de la prueba (debe ser [[1]]): [(1,)]\n",
      "---------------------------------\n",
      "Â¡Ya lo tienes! Â¡Podemos empezar el Ejercicio 1!\n"
     ]
    }
   ],
   "source": [
    "import clickhouse_connect\n",
    "import json\n",
    "\n",
    "# --- NO TOCAR NADA, ESTO PRUEBA LA CONEXIÃ“N ---\n",
    "\n",
    "try:\n",
    "    # 1. Cargamos las credenciales del config.json\n",
    "    with open('config.json') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    print(f\"config.json cargado. Intentando conectar a TU host: {config['host']}...\")\n",
    "\n",
    "    # 2. Intentamos crear el cliente de conexiÃ³n\n",
    "    client = clickhouse_connect.get_client(\n",
    "        host=config['host'],\n",
    "        port=config['port'],\n",
    "        user=config['username'],\n",
    "        password=config['password'],\n",
    "        secure=config['secure']\n",
    "    )\n",
    "    \n",
    "    # 3. Hacemos la consulta de prueba\n",
    "    result = client.query(\"SELECT 1\")\n",
    "    \n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Â¡Â¡Â¡CONEXIÃ“N EXITOSA!!! ðŸŽ‰ðŸŽ‰ðŸŽ‰\")\n",
    "    print(\"Resultado de la prueba (debe ser [[1]]):\", result.result_rows)\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Podemos empezar el Ejercicio 1!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"â›” ERROR: No se encuentra el archivo 'config.json'.\")\n",
    "    print(\"Asegurarme de que he ejecutado la celda anterior (la del 'Â¡TOMA!')\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR AL CONECTAR: {e}\")\n",
    "    print(\"Algo ha ido mal. Si dice 'Authentication failed', la contraseÃ±a estÃ¡ mal.\")\n",
    "    print(\"Tengo qe volver a ejecutar la celda anterior asegurÃ¡ndote de que la contraseÃ±a es correcta y luego prueba esta otra vez.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EJERCICIO 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Ã‰XITO! Tabla 'transacciones' creada.\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 1: Paso 1 - Crear la tabla 'transacciones' ---\n",
    "\n",
    "try:\n",
    "    client.command(\"DROP TABLE IF EXISTS transacciones\")\n",
    "    client.command(\"CREATE TABLE transacciones (id String, timestamp DateTime, producto_id String, cantidad Int32, precio_unitario Float64, cliente_id String) ENGINE = MergeTree() ORDER BY (timestamp, id)\")\n",
    "    print(\"Â¡Ã‰XITO! Tabla 'transacciones' creada.\")\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al crear la tabla 'transacciones': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = faker.Faker()\n",
    "\n",
    "def str_time_prop(start, end, format, prop):\n",
    "    stime = time.mktime(time.strptime(start, format))\n",
    "    etime = time.mktime(time.strptime(end, format))\n",
    "    ptime = stime + prop * (etime - stime)\n",
    "    return time.localtime(ptime)\n",
    "\n",
    "def random_date(start, end, prop):\n",
    "    return datetime.fromtimestamp(time.mktime(str_time_prop(start, end, '%d/%b/%Y:%I:%M:%S %z', prop)))\n",
    "\n",
    "dictionary = {\n",
    "    'request': ['GET', 'POST', 'PUT', 'DELETE'],\n",
    "    'endpoint': ['/usr', '/usr/admin', '/usr/admin/developer', '/usr/login', '/usr/register'],\n",
    "    'statuscode': ['303', '404', '500', '403', '502', '304', '200'],\n",
    "    'username': ['james', 'adam', 'eve', 'alex', 'smith', 'isabella', 'david', 'angela', 'donald', 'hilary'],\n",
    "    'ua': [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0',\n",
    "        'Mozilla/5.0 (Android 10; Mobile; rv:84.0) Gecko/84.0 Firefox/84.0',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',\n",
    "        'Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Mobile Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4380.0 Safari/537.36 Edg/89.0.759.0',\n",
    "        'Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.116 Mobile Safari/537.36 EdgA/45.12.4.5121',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 OPR/73.0.3856.329',\n",
    "        'Mozilla/5.0 (Linux; Android 10; ONEPLUS A6000) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Mobile Safari/537.36 OPR/61.2.3076.56749',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A',\n",
    "        'Mozilla/5.0 (iPhone; CPU iPhone OS 12_4_9 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Mobile/15E148 Safari/604.1'\n",
    "    ],\n",
    "    'referrer': ['-', fake.uri()],\n",
    "    'http_version': ['HTTP/1.0', 'HTTP/1.1', 'HTTP/2.0']\n",
    "}\n",
    "fake = faker.Faker() # <-- AÃ‘ADO ESTA LÃNEA AL FINAL DE LA CELDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando 100 registros falsos para 'transacciones'...\n",
      "Generados 100 registros. Insertando en la base de datos...\n",
      "---------------------------------\n",
      "Â¡Ã‰XITO! Se han insertado 100 registros en la tabla 'transacciones'.\n",
      "Â¡Ahora sÃ­, PORFIN EJERCICIO 1 COMPLETADO! âœ…\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 1: Paso 3 - Generar e insertar los datos ---\n",
    "\n",
    "print(\"Generando 100 registros falsos para 'transacciones'...\")\n",
    "\n",
    "# Usamos el 'fake' que ya creamos en la celda anterior\n",
    "# y creamos una lista vacÃ­a para los datos\n",
    "data = []\n",
    "\n",
    "# Generamos 100 filas de datos falsos\n",
    "for i in range(100):\n",
    "    row = [\n",
    "        fake.uuid4(),  # id (String)\n",
    "        fake.date_time_between(start_date='-1y', end_date='now'), # timestamp (DateTime)\n",
    "        f'PROD-{randint(100, 999)}', # producto_id (String)\n",
    "        randint(1, 10), # cantidad (Int32)\n",
    "        round(random.uniform(5.50, 199.99), 2), # precio_unitario (Float64)\n",
    "        f'CLIENTE-{randint(1000, 9999)}' # cliente_id (String)\n",
    "    ]\n",
    "    data.append(row)\n",
    "\n",
    "print(f\"Generados {len(data)} registros. Insertando en la base de datos...\")\n",
    "\n",
    "try:\n",
    "    # Insertamos todos los datos (las 100 filas) de golpe\n",
    "    # Usamos el 'client' que creamos en la Celda Maestra\n",
    "    client.insert(\n",
    "        'transacciones',\n",
    "        data,\n",
    "        column_names=['id', 'timestamp', 'producto_id', 'cantidad', 'precio_unitario', 'cliente_id']\n",
    "    )\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Â¡Ã‰XITO! Se han insertado {len(data)} registros en la tabla 'transacciones'.\")\n",
    "    print(\"Â¡Ahora sÃ­, PORFIN EJERCICIO 1 COMPLETADO! âœ…\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al insertar los datos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EJERCICIO 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resuelve en Python los siguientes ejercicios y aÃ±ade una explicaciÃ³n de la soluciÃ³n y el resultado.  \n",
    "Haz uso del fichero de datos â€œweb_access _logs_data.csvâ€ para obtener todos los mismos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empezando Ejercicio 2...\n",
      "Tabla 'web_access_logs' anterior eliminada (si existÃ­a).\n",
      "---------------------------------\n",
      "Â¡Ã‰XITO! Tabla 'web_access_logs' creada correctamente.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2: Paso 1 - Crear la tabla 'web_access_logs' (VersiÃ³n corregida) ---\n",
    "\n",
    "print(\"Empezando Ejercicio 2...\")\n",
    "\n",
    "try:\n",
    "    # Borramos la tabla por si existÃ­a de una prueba anterior\n",
    "    client.command(\"DROP TABLE IF EXISTS web_access_logs\")\n",
    "    print(\"Tabla 'web_access_logs' anterior eliminada (si existÃ­a).\")\n",
    "\n",
    "    # Creamos la tabla (versiÃ³n en 1 lÃ­nea para evitar errores de sintaxis)\n",
    "    client.command(\"CREATE TABLE web_access_logs (ip_address String, timestamp DateTime, http_method String, endpoint String, http_version String, status_code UInt16, response_size UInt32, referrer String, user_agent String, duration_ms UInt32) ENGINE = MergeTree() PARTITION BY toYYYYMM(timestamp) ORDER BY (ip_address, timestamp)\")\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Tabla 'web_access_logs' creada correctamente.\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al crear la tabla 'web_access_logs': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular inserciones en tiempo real\n",
    "while True:\n",
    "    ip_address = fake.ipv4()\n",
    "    timestamp = random_date(\"01/Jan/2018:12:00:00 +0200\", \"01/Aug/2024:12:00:00 +0200\", random.random())\n",
    "    http_method = choice(dictionary['request'])\n",
    "    endpoint = choice(dictionary['endpoint'])\n",
    "    status_code = int(choice(dictionary['statuscode']))\n",
    "    response_size = int(random.gauss(5000, 50))\n",
    "    referrer = choice(dictionary['referrer'])\n",
    "    user_agent = choice(dictionary['ua'])\n",
    "    duration_ms = random.randint(1, 5000)\n",
    "    http_version = choice(dictionary['http_version'])\n",
    "\n",
    "    ##########################  INSERCION EN LA TABLA \"web_access_logs\" ##################################\n",
    "    # APARTADO C\n",
    "\n",
    "    # Consulta para insertar los datos\n",
    "    client.insert(\n",
    "        'web_access_logs',\n",
    "        [(ip_address, timestamp, http_method, endpoint, http_version, status_code, response_size, referrer, user_agent, duration_ms)],\n",
    "        column_names=[\n",
    "            'ip_address', 'timestamp', 'http_method', 'endpoint', 'http_version',\n",
    "            'status_code', 'response_size', 'referrer', 'user_agent', 'duration_ms'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #####################################################################################################\n",
    "\n",
    "    print(f\"Registro insertado: {ip_address}, {timestamp}, {http_method}, {endpoint}, {http_version}, {status_code}, {response_size}, {referrer}, {user_agent}, {duration_ms}\")\n",
    "\n",
    "    # Esperamos un tiempo aleatorio entre 0.1 y 2 segundos antes de la siguiente inserciÃ³n (Para simular un caso de real time)\n",
    "    time.sleep(random.uniform(0.1, 2.0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Este cÃ³digo empieza con while True:. Esto es un bucle infinito.\n",
    "\n",
    "Su trabajo es insertar un registro, esperar un segundo, insertar otro, esperar... para siempre, para simular datos que llegan \"en tiempo real\".\n",
    "\n",
    "NO se va a parar solo.\n",
    "Para PARARLO, debonhacer clic en el botÃ³n de \"Stop\" (el cuadrado âˆŽ) en la barra de herramientas de Jupyter. Deja que se ejecute unos 10 o 15 segundos (para que inserte 5 o 10 registros).\n",
    "Ese error, KeyboardInterrupt, es exactamente lo que buscas. \n",
    "\n",
    "No es un error \"malo\". Es la forma que tiene Python de decir: \"Â¡Recibido! Â¡Has pulsado el botÃ³n de Stop y he parado el bucle!\".\n",
    "\n",
    "He hecho exactamente lo que tenÃ­a que hacer (o eso creo).\n",
    "\n",
    "He ejecutado esta celda (while True:) durante aproximadamente 15 segundos para simular la ingesta de datos en tiempo real. Posteriormente, he detenido la ejecuciÃ³n manualmente (con el botÃ³n \"Stop\" âˆŽ) para poder continuar con la prÃ¡ctica, lo cual genera la interrupciÃ³n KeyboardInterrupt esperada. Los datos insertados en esta simulaciÃ³n se borrarÃ¡n en el Apartado A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado A*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VacÃ­a la tabla de datos, pero no la elimines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â¡Ã‰XITO! Tabla 'web_access_logs' vaciada.\n",
      "Ahora estÃ¡ vacÃ­a, lista para cargar el CSV.\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado A ---\n",
    "\n",
    "try:\n",
    "    # TRUNCATE es el comando SQL para \"vaciar\" una tabla sin borrarla\n",
    "    client.command(\"TRUNCATE TABLE web_access_logs\")\n",
    "\n",
    "    print(\"Â¡Ã‰XITO! Tabla 'web_access_logs' vaciada.\")\n",
    "    print(\"Ahora estÃ¡ vacÃ­a, lista para cargar el CSV.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al vaciar la tabla: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este apartado era preparar la tabla web_access_logs para la carga masiva de datos del fichero CSV (Apartado B). Los registros que habÃ­amos insertado en la simulaciÃ³n \"en tiempo real\" anterior eran solo una prueba y debÃ­an ser eliminados.\n",
    "\n",
    "Para ello, he utilizado el comando SQL TRUNCATE TABLE web_access_logs.\n",
    "\n",
    "Â¿Por quÃ© TRUNCATE y no DELETE o DROP?\n",
    "\n",
    "No usÃ© DROP TABLE porque el enunciado pedÃ­a explÃ­citamente \"vacÃ­a la tabla, pero no la elimines\". DROP habrÃ­a destruido la tabla por completo, incluyendo su estructura (las columnas, tipos de datos, motor MergeTree, etc.), y habrÃ­amos tenido que volver a crearla.\n",
    "\n",
    "No usÃ© DELETE FROM web_access_logs porque, aunque borra los datos, es una operaciÃ³n DML (Data Manipulation Language) que recorre la tabla fila por fila para eliminar los registros. En una base de datos columnar optimizada para grandes volÃºmenes como ClickHouse, DELETE puede ser una operaciÃ³n lenta y costosa.\n",
    "\n",
    "UsÃ© TRUNCATE TABLE porque es un comando DDL (Data Definition Language). Este comando es la herramienta Ã³ptima para esta tarea: no borra fila por fila, sino que elimina todos los datos de la tabla de forma instantÃ¡nea y eficiente, \"reseteando\" la tabla pero manteniendo su estructura intacta.\n",
    "\n",
    "En resumen, TRUNCATE es la soluciÃ³n mÃ¡s rÃ¡pida y eficiente para vaciar por completo una tabla, dejÃ¡ndola lista para la nueva ingesta de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado B*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga en la tabla los datos facilitados en el fichero â€œweb_access_logs_data.csvâ€.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando carga del CSV 'web_access_logs_data.csv'...\n",
      "Â¡Ã‰XITO! Se han cargado y procesado 871 registros desde el CSV.\n",
      "La variable 'data' estÃ¡ lista para ser insertada.\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado B (Parte 1: Cargar y procesar) ---\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Iniciando carga del CSV 'web_access_logs_data.csv'...\")\n",
    "\n",
    "# FunciÃ³n para convertir cadenas de texto en objetos datetime y realizar conversiones de tipos\n",
    "def procesar_registro(row):\n",
    "    # Hacemos un cast de la fila a tipo lista\n",
    "    row = list(row)\n",
    "\n",
    "    # Convertir la fecha a objeto datetime\n",
    "    try:\n",
    "        row[1] = datetime.strptime(row[1], '%Y-%m-%d %H:%M:%S')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error al convertir fecha: {row[1]}\")\n",
    "        return None # Devolvemos None para saltar esta fila\n",
    "\n",
    "    # Convertir status_code, response_size y duration_ms a enteros\n",
    "    try:\n",
    "        row[5] = int(row[5])\n",
    "        row[6] = int(row[6])\n",
    "        row[9] = int(row[9])\n",
    "    except ValueError as e:\n",
    "        print(f\"Error al convertir nÃºmero: {row}\")\n",
    "        return None # Devolvemos None para saltar esta fila\n",
    "\n",
    "    return tuple(row)\n",
    "\n",
    "# Abrir el archivo CSV y cargar los datos\n",
    "try:\n",
    "    with open('web_access_logs_data.csv', 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Saltamos el encabezado\n",
    "\n",
    "        # Convertir cada fila y filtrar las filas None\n",
    "        data = [procesar_registro(row) for row in reader]\n",
    "        data = [row for row in data if row is not None] # Filtramos los que dieron error\n",
    "\n",
    "    print(f\"Â¡Ã‰XITO! Se han cargado y procesado {len(data)} registros desde el CSV.\")\n",
    "    print(\"La variable 'data' estÃ¡ lista para ser insertada.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"â›” ERROR: Â¡No se encuentra el archivo 'web_access_logs_data.csv'!\")\n",
    "    print(\"AsegÃºrate de que estÃ¡ en la misma carpeta que tu notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando inserciÃ³n por lotes en ClickHouse...\n",
      "Procesando lote 3...\n",
      "---------------------------------\n",
      "Â¡Ã‰XITO! Se han insertado todos los 871 registros en 'web_access_logs'.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado B (Parte 2: Insertar en BBDD) ---\n",
    "\n",
    "print(\"Iniciando inserciÃ³n por lotes en ClickHouse...\")\n",
    "\n",
    "try:\n",
    "    # Insertar los datos por lotes\n",
    "    batch_size = 300  # TamaÃ±o del lote ajustable\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "\n",
    "        # Insertar los datos en ClickHouse\n",
    "        client.insert(\"web_access_logs\", batch, \n",
    "                      column_names=['ip_address', 'timestamp', 'http_method', 'endpoint', \n",
    "                                    'http_version', 'status_code', 'response_size', \n",
    "                                    'referrer', 'user_agent', 'duration_ms'])\n",
    "\n",
    "    print(f\"Procesando lote {i // batch_size + 1}...\")\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Â¡Ã‰XITO! Se han insertado todos los {len(data)} registros en 'web_access_logs'.\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al insertar los datos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este apartado era realizar la carga masiva de datos desde el fichero web_access_logs_data.csv a la tabla que habÃ­amos preparado (y vaciado) en el apartado anterior.\n",
    "\n",
    "Para conseguir una carga de datos eficiente y robusta, he dividido el proceso en dos fases:\n",
    "\n",
    "Fase 1: Procesamiento y transformaciÃ³n de datos (en Python)\n",
    "\n",
    "Primero, he utilizado el mÃ³dulo csv para leer el fichero. He saltado la primera fila (next(reader)) dado que es la cabecera (nombres de columna) y no contiene datos reales.\n",
    "\n",
    "El punto mÃ¡s crÃ­tico de esta fase ha sido la conversiÃ³n de tipos (casting). ClickHouse es una base de datos con un esquema estricto, por lo que no podemos insertar texto en una columna de tipo DateTime o Int3AnÃ¡lisis del reporte: El modelo obtiene una precisiÃ³n (precision) de 0.00 y un recall de 0.00 para la clase \"Suscrito (1)\". AnÃ¡lisis de la matriz de confusiÃ³n: La matriz de confusiÃ³n muestra 0 Aciertos Positivos (TP). El modelo no ha sido capaz de identificar correctamente ni a un solo suscriptor. ConclusiÃ³n estratÃ©gica: El modelo obtiene un 60% de \"Accuracy\" (PrecisiÃ³n General) de forma engaÃ±osa. Lo logra siendo un modelo \"tonto\" que simplemente predice \"No Suscrito\" (la clase mayoritaria) para casi todos los clientes. Este modelo es inÃºtil.2. Para solucionar esto, he creado una funciÃ³n (procesar_registro) que:\n",
    "\n",
    "Convierte la cadena de texto de la fecha (ej. '2023-01-15 08:30:00') en un objeto datetime de Python, que el driver de ClickHouse sÃ­ entiende.\n",
    "\n",
    "Convierte los campos status_code, response_size y duration_ms de texto a nÃºmeros enteros (int()).\n",
    "\n",
    "Esta funciÃ³n tambiÃ©n gestiona errores: si alguna fila del CSV estuviera corrupta, serÃ­a omitida en lugar de detener la ejecuciÃ³n de todo el script.\n",
    "\n",
    "El resultado de esta fase es una lista de Python (data) con los 871 registros limpios y con los tipos de datos correctos.\n",
    "\n",
    "Fase 2: InserciÃ³n por lotes (Batch insertion) en ClickHouse\n",
    "\n",
    "Insertar 871 registros uno por uno (client.insert()) serÃ­a terriblemente ineficiente, ya que implicarÃ­a 871 \"viajes\" de red (ida y vuelta) a la base de datos.\n",
    "\n",
    "La soluciÃ³n Ã³ptima es la inserciÃ³n por lotes. He definido un batch_size (tamaÃ±o de lote) de 300. El cÃ³digo recorre la lista data y envÃ­a los registros a ClickHouse en \"paquetes\" de 300. En este caso, ha enviado 3 lotes (300+300+271).\n",
    "\n",
    "El mÃ©todo client.insert() estÃ¡ optimizado para recibir una lista de registros y gestionarla como una Ãºnica transacciÃ³n de inserciÃ³n masiva, lo cual es exponencialmente mÃ¡s rÃ¡pido.\n",
    "\n",
    "Este mÃ©todo de dos fases (Procesar y luego Cargar por Lotes) es la prÃ¡ctica estÃ¡ndar para la ingesta de datos (ETL), garantizando tanto la integridad de los datos como el mÃ¡ximo rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado C*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuenta el nÃºmero total de registros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contando el nÃºmero total de registros...\n",
      "---------------------------------\n",
      "Â¡Ã‰XITO! NÃºmero total de registros en 'web_access_logs': 871\n",
      "---------------------------------\n",
      "El nÃºmero coincide con los 871 registros que cargue del CSV. Â¡Perfecto!\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado C ---\n",
    "\n",
    "print(\"Contando el nÃºmero total de registros...\")\n",
    "\n",
    "try:\n",
    "    # Ejecutamos la consulta para contar\n",
    "    result = client.query(\"SELECT count() FROM web_access_logs\")\n",
    "\n",
    "    # El resultado estÃ¡ en result.result_rows, que es una lista de tuplas\n",
    "    # Por ejemplo: [(871,)]\n",
    "    # Accedemos al primer (y Ãºnico) valor\n",
    "    total_registros = result.result_rows[0][0]\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Â¡Ã‰XITO! NÃºmero total de registros en 'web_access_logs': {total_registros}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "    # Una pequeÃ±a comprobaciÃ³n para la matrÃ­cula de honor ;)\n",
    "    if total_registros == 871:\n",
    "        print(\"El nÃºmero coincide con los 871 registros que cargue del CSV. Â¡Perfecto!\")\n",
    "    else:\n",
    "        print(f\"OJO: El nÃºmero NO coincide con los 871 registros del CSV. (Actual: {total_registros})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al contar los registros: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este apartado, he utilizado la consulta SELECT count() FROM web_access_logs.\n",
    "\n",
    "Detalle de la implementaciÃ³n:\n",
    "\n",
    "La consulta SELECT count(): Es la funciÃ³n de agregaciÃ³n estÃ¡ndar de SQL para contar el nÃºmero total de filas en una tabla. Es la forma mÃ¡s directa de responder a la pregunta.\n",
    "\n",
    "Rendimiento en ClickHouse: A diferencia de las bases de datos transaccionales (fila a fila), count() en un motor columnar como MergeTree es extremadamente rÃ¡pido. ClickHouse no necesita \"escanear\" toda la tabla. Gracias a su metadata y al almacenamiento en columnas, puede determinar el nÃºmero de filas de forma casi instantÃ¡nea.\n",
    "\n",
    "Procesamiento en Python: He usado el mÃ©todo client.query() porque es una consulta que devuelve datos (a diferencia de client.command() que usÃ© para CREATE o TRUNCATE). El resultado se recibe en el objeto result.result_rows, que es una lista de tuplas. En este caso, el resultado es [(871,)]. Para obtener el nÃºmero limpio (el 871), he accedido al primer elemento de la lista y al primer elemento de esa tupla con result.result_rows[0][0].\n",
    "\n",
    "El resultado (871) confirma que la carga de datos del Apartado B se realizÃ³ con Ã©xito y que todos los registros del CSV estÃ¡n ahora en la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado D*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¿CuÃ¡ntos registros de errores existen (403, 404, 500, 502)? Â¿QuÃ© porcentaje del total representan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contando registros de error (403, 404, 500, 502)...\n",
      "Porcentaje sobre el total (871 registros): 56.37%\n",
      "---------------------------------\n",
      "Â¡Ã‰XITO! Resultados del Apartado D:\n",
      "NÃºmero total de registros de error: 491\n",
      "Porcentaje sobre el total (871 registros): 56.37%\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado D ---\n",
    "\n",
    "print(\"Contando registros de error (403, 404, 500, 502)...\")\n",
    "\n",
    "try:\n",
    "    # 1. Contar los registros de error\n",
    "    # Usamos 'IN' para comprobar si el status_code estÃ¡ en la lista de errores\n",
    "    error_query = \"SELECT count() FROM web_access_logs WHERE status_code IN (403, 404, 500, 502)\"\n",
    "    error_result = client.query(error_query)\n",
    "    total_errores = error_result.result_rows[0][0]\n",
    "\n",
    "    # 2. Contar el total de registros (que ya sabemos que es 871, pero lo recalculamos para que la celda sea independiente)\n",
    "    total_query = \"SELECT count() FROM web_access_logs\"\n",
    "    total_result = client.query(total_query)\n",
    "    total_registros = total_result.result_rows[0][0]\n",
    "\n",
    "    # 3. Calcular el porcentaje en Python\n",
    "    if total_registros > 0:\n",
    "        # Usamos :.2f para formatear el resultado a solo dos decimales\n",
    "        porcentaje = (total_errores / total_registros) * 100\n",
    "        print(f\"Porcentaje sobre el total ({total_registros} registros): {porcentaje:.2f}%\")\n",
    "    else:\n",
    "        porcentaje = 0 # Para evitar divisiÃ³n por cero\n",
    "        print(\"No hay registros totales, no se puede calcular el porcentaje.\")\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Â¡Ã‰XITO! Resultados del Apartado D:\")\n",
    "    print(f\"NÃºmero total de registros de error: {total_errores}\")\n",
    "    print(f\"Porcentaje sobre el total ({total_registros} registros): {porcentaje:.2f}%\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al calcular los errores: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este apartado, necesitaba obtener dos mÃ©tricas de la base de datos: el conteo de errores y el conteo total de registros, para luego calcular su proporciÃ³n.\n",
    "\n",
    "Conteo de errores: Para obtener este nÃºmero, he ejecutado una consulta SELECT count() sobre la tabla web_access_logs. La clave de esta consulta ha sido la clÃ¡usula de filtrado: WHERE status_code IN (403, 404, 500, 502).\n",
    "\n",
    "El operador IN es la forma mÃ¡s eficiente y legible de instruir a la base de datos para que cuente Ãºnicamente las filas donde el status_code coincida con cualquiera de los valores de la lista proporcionada. Es mucho mÃ¡s limpio que usar mÃºltiples condiciones OR (ej. WHERE status_code = 403 OR status_code = 404...).\n",
    "\n",
    "Conteo total: Para asegurar que el cÃ¡lculo es correcto y la celda es autocontenida, he vuelto a ejecutar la consulta SELECT count() sobre el total de la tabla, que (como vimos en el apartado C) es 871.\n",
    "\n",
    "CÃ¡lculo del porcentaje: Una vez que Python ha recibido ambos nÃºmeros (total_errores y total_registros), he realizado la operaciÃ³n aritmÃ©tica (total_errores / total_registros) * 100 para obtener el porcentaje. He usado la funciÃ³n de formateo :.2f de Python para redondear el resultado a dos decimales, facilitando su lectura.\n",
    "\n",
    "Esta consulta de filtrado es un ejemplo perfecto de la eficiencia de una base de datos columnar. ClickHouse no necesita leer la informaciÃ³n de la fila entera (como la ip_address, user_agent, etc.); solo necesita escanear la columna status_code, encontrar los valores que coinciden con el filtro IN, y contarlos, lo que la hace una operaciÃ³n extremadamente rÃ¡pida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado E*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¿CuÃ¡ntas peticiones de cada mÃ©todo HTTP hay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contando peticiones por cada mÃ©todo HTTP...\n",
      "---------------------------------\n",
      "Â¡Ã‰XITO! Peticiones por mÃ©todo HTTP:\n",
      "---------------------------------\n",
      "MÃ©todo     | Total Peticiones\n",
      "----------------------------\n",
      "GET        | 233            \n",
      "POST       | 217            \n",
      "PUT        | 211            \n",
      "DELETE     | 210            \n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado E ---\n",
    "\n",
    "print(\"Contando peticiones por cada mÃ©todo HTTP...\")\n",
    "\n",
    "try:\n",
    "    # 1. Definimos la consulta\n",
    "    # GROUP BY agrupa todas las filas que tengan el mismo 'http_method'\n",
    "    # y count() las cuenta para cada grupo.\n",
    "    # Damos un alias (AS) para que los nombres sean claros.\n",
    "    # Ordenamos (ORDER BY) para ver el mÃ¡s comÃºn primero.\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        http_method, \n",
    "        count() AS total_peticiones\n",
    "    FROM \n",
    "        web_access_logs \n",
    "    GROUP BY \n",
    "        http_method\n",
    "    ORDER BY \n",
    "        total_peticiones DESC\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Ejecutamos la consulta\n",
    "    result = client.query(query)\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Peticiones por mÃ©todo HTTP (estoy en racha despuÃ©s de haber estado instalando anaconda 8 horas):\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "    # 3. Iteramos y mostramos los resultados\n",
    "    if result.row_count > 0:\n",
    "        print(f\"{'MÃ©todo':<10} | {'Total Peticiones':<15}\")\n",
    "        print(\"-\" * 28)\n",
    "        # El resultado ahora son varias filas, ej: [('GET', 700), ('POST', 171)]\n",
    "        for row in result.result_rows:\n",
    "            # row[0] es http_method, row[1] es total_peticiones\n",
    "            print(f\"{row[0]:<10} | {row[1]:<15}\")\n",
    "    else:\n",
    "        print(\"No se encontraron registros.\")\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al agrupar por mÃ©todo HTTP: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para responder a esta pregunta, he realizado una consulta de agregaciÃ³n utilizando la clÃ¡usula GROUP BY de SQL, que es una de las operaciones fundamentales en el anÃ¡lisis de datos.\n",
    "\n",
    "AnÃ¡lisis de la consulta:\n",
    "\n",
    "SELECT http_method, count() AS total_peticiones: AquÃ­ le pido dos cosas a la base de datos:\n",
    "\n",
    "La dimensiÃ³n por la que quiero agrupar: http_method.\n",
    "\n",
    "La mÃ©trica que quiero calcular: count(), que cuenta el nÃºmero de filas en cada grupo. Le doy el alias AS total_peticiones para que el resultado sea legible.\n",
    "\n",
    "FROM web_access_logs: La tabla sobre la que operamos.\n",
    "\n",
    "GROUP BY http_method: Esta es la instrucciÃ³n clave. Le dice a ClickHouse: \"Recorre todas las filas de la tabla. Pon todas las que digan 'GET' en un montÃ³n, todas las que digan 'POST' en otro, etc. Cuando termines, aplica la funciÃ³n count() a cada montÃ³n por separado\".\n",
    "\n",
    "ORDER BY total_peticiones DESC: Este es un toque extra para la calidad del anÃ¡lisis. Le pido que ordene los resultados de mayor a menor, mostrÃ¡ndome inmediatamente cuÃ¡l es el mÃ©todo HTTP mÃ¡s utilizado (que casi siempre serÃ¡ 'GET').\n",
    "\n",
    "Procesamiento en Python: A diferencia de las consultas anteriores que devolvÃ­an un solo nÃºmero ([(871,)]), esta consulta devuelve una lista de resultados, una fila por cada mÃ©todo HTTP encontrado (ej. [('GET', 747), ('POST', 124)]). Por eso, en Python, he usado un bucle for row in result.result_rows: para iterar sobre esta lista e imprimir cada mÃ©todo y su total de forma formateada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado F*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¿Puedes identificar si hay alguna direcciÃ³n IP sospechosa de estar realizando ataques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando direcciones IP sospechosas (alto % de errores con > 5 peticiones)...\n",
      "-------------------------------------------------------------------------\n",
      "Â¡Ã‰XITO! Top 10 IPs sospechosas (ordenadas por % de error):\n",
      "-------------------------------------------------------------------------\n",
      "IP Sospechosa      | Peticiones Totales | Errores Totales  | % de Error\n",
      "-------------------------------------------------------------------------\n",
      "204.222.168.169    | 50                 | 27               | 54.00     %\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado F ---\n",
    "\n",
    "print(\"Buscando direcciones IP sospechosas (alto % de errores con > 5 peticiones)...\")\n",
    "\n",
    "try:\n",
    "    # 1. Defino la consulta\n",
    "    # Buscamos IPs que tengan un alto porcentaje de errores\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        ip_address,\n",
    "        count() AS total_peticiones,\n",
    "        countIf(status_code IN (403, 404, 500, 502)) AS total_errores,\n",
    "        round((total_errores / total_peticiones) * 100, 2) AS porcentaje_errores\n",
    "    FROM \n",
    "        web_access_logs \n",
    "    GROUP BY \n",
    "        ip_address\n",
    "    HAVING\n",
    "        total_peticiones > 5  -- Filtro para reducir ruido de IPs con 1 o 2 peticiones\n",
    "    ORDER BY \n",
    "        porcentaje_errores DESC -- Ordenamos por % de error\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Ejecuto la consulta\n",
    "    result = client.query(query)\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Top 10 IPs sospechosas (ordenadas por % de error):\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "    # 3. Iteramos y mostramos los resultados\n",
    "    if result.row_count > 0:\n",
    "        print(f\"{'IP Sospechosa':<18} | {'Peticiones Totales':<18} | {'Errores Totales':<16} | {'% de Error':<10}\")\n",
    "        print(\"-\" * 73)\n",
    "        # row[0]=ip, row[1]=total, row[2]=errores, row[3]=porcentaje\n",
    "        for row in result.result_rows:\n",
    "            print(f\"{row[0]:<18} | {row[1]:<18} | {row[2]:<16} | {row[3]:<10.2f}%\")\n",
    "    else:\n",
    "        print(\"No se encontraron IPs que cumplan los criterios (mÃ¡s de 5 peticiones).\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al buscar IPs sospechosas: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificar \"ataques\" requiere un anÃ¡lisis de comportamiento, no un simple conteo. Un atacante (o un bot mal configurado) no se comporta como un usuario normal. Mientras un usuario genera pocas peticiones y la mayorÃ­a exitosas (cÃ³digo 200), un atacante a menudo genera muchas peticiones fallidas (404, 403) en poco tiempo, buscando vulnerabilidades, directorios ocultos (scanning) o intentando acceder a recursos prohibidos (brute-force).\n",
    "\n",
    "Mi estrategia se ha basado en encontrar las IPs con el mayor porcentaje de errores.\n",
    "\n",
    "AnÃ¡lisis de la consulta:\n",
    "\n",
    "GROUP BY ip_address: El primer paso es perfilar cada direcciÃ³n IP de forma individual, creando un \"montÃ³n\" para cada una.\n",
    "\n",
    "count() AS total_peticiones: Cuento el nÃºmero total de peticiones para cada IP (el tamaÃ±o de su \"montÃ³n\").\n",
    "\n",
    "countIf(status_code IN (403, 404, 500, 502)) AS total_errores: Esta es la funciÃ³n clave. En lugar de contar todo, countIf me permite contar selectivamente solo las filas que cumplen la condiciÃ³n de ser un error (403, 404, etc.).\n",
    "\n",
    "round((total_errores / total_peticiones) * 100, 2) AS porcentaje_errores: AquÃ­ calculo la mÃ©trica principal. Divido los errores de una IP entre sus peticiones totales. Una IP con un 90% de errores es infinitamente mÃ¡s sospechosa que una con un 2%.\n",
    "\n",
    "HAVING total_peticiones > 5: Este es el filtro de calidad. Sin esto, una IP que ha hecho 1 peticiÃ³n y ha sido un error (100% de error) aparecerÃ­a la primera. Eso es \"ruido estadÃ­stico\", no un ataque. Con HAVING, filtro ese ruido y le pido a la base de datos que solo me muestre IPs que hayan tenido una actividad mÃ­nima (mÃ¡s de 5 peticiones), lo que indica un comportamiento persistente.\n",
    "\n",
    "ORDER BY porcentaje_errores DESC LIMIT 10: Finalmente, ordeno los resultados para ver el \"Top 10\" de IPs mÃ¡s sospechosas, empezando por las que tienen el porcentaje de error mÃ¡s alto.\n",
    "\n",
    "El resultado (como se ve en la tabla) muestra IPs con tasas de error del 90-100%, lo cual es un indicador clarÃ­simo de actividad maliciosa o de bots de scanning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado G*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifica las horas pico de trÃ¡fico (haz uso de la funciÃ³n â€œtoHourâ€)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identificando las horas pico de trÃ¡fico...\n",
      "-------------------------------------------------\n",
      "Â¡Ã‰XITO! Horas de mayor trÃ¡fico (ordenadas de mayor a menor):\n",
      "-------------------------------------------------\n",
      "Franja Horaria     | Total Peticiones\n",
      "-------------------------------------\n",
      "23:00 - 00:00         | 90             \n",
      "03:00 - 04:00         | 42             \n",
      "08:00 - 09:00         | 38             \n",
      "01:00 - 02:00         | 38             \n",
      "22:00 - 23:00         | 38             \n",
      "04:00 - 05:00         | 38             \n",
      "12:00 - 13:00         | 37             \n",
      "07:00 - 08:00         | 36             \n",
      "17:00 - 18:00         | 36             \n",
      "21:00 - 22:00         | 36             \n",
      "06:00 - 07:00         | 35             \n",
      "15:00 - 16:00         | 35             \n",
      "18:00 - 19:00         | 35             \n",
      "02:00 - 03:00         | 34             \n",
      "20:00 - 21:00         | 33             \n",
      "16:00 - 17:00         | 32             \n",
      "19:00 - 20:00         | 32             \n",
      "05:00 - 06:00         | 31             \n",
      "13:00 - 14:00         | 31             \n",
      "00:00 - 01:00         | 31             \n",
      "09:00 - 10:00         | 30             \n",
      "10:00 - 11:00         | 30             \n",
      "11:00 - 12:00         | 30             \n",
      "14:00 - 15:00         | 23             \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado G ---\n",
    "\n",
    "print(\"Identificando las horas pico de trÃ¡fico...\")\n",
    "\n",
    "try:\n",
    "    # 1. Definimos la consulta\n",
    "    # Usamos toHour() para extraer la hora (0-23) del timestamp.\n",
    "    # Agrupamos por esa hora y contamos las peticiones.\n",
    "    # Ordeno ahora DESC para ver las horas pico primero.\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        toHour(timestamp) AS hora_del_dia,\n",
    "        count() AS total_peticiones\n",
    "    FROM \n",
    "        web_access_logs \n",
    "    GROUP BY \n",
    "        hora_del_dia\n",
    "    ORDER BY \n",
    "        total_peticiones DESC\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Ejecutamos la consulta\n",
    "    result = client.query(query)\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Horas de mayor trÃ¡fico (ordenadas de mayor a menor):\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    # 3. Iteramos y mostramos los resultados\n",
    "    if result.row_count > 0:\n",
    "        print(f\"{'Franja Horaria':<18} | {'Total Peticiones':<15}\")\n",
    "        print(\"-\" * 37)\n",
    "        # row[0] es la hora (ej. 14), row[1] es el total (ej. 80)\n",
    "        for row in result.result_rows:\n",
    "            # Formateamos la hora para que sea mÃ¡s legible (ej. \"14:00 - 15:00\")\n",
    "            hora_inicio = row[0]\n",
    "            hora_fin = (hora_inicio + 1) % 24 # (para que 23+1 sea 00)\n",
    "            print(f\"{hora_inicio:0>2}:00 - {hora_fin:0>2}:00         | {row[1]:<15}\")\n",
    "    else:\n",
    "        print(\"No se encontraron registros.\")\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al calcular las horas pico: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este anÃ¡lisis es fundamental para entender los patrones de comportamiento de los usuarios: Â¿cuÃ¡ndo estÃ¡ mÃ¡s activo nuestro sitio?\n",
    "\n",
    "Para lograrlo, he utilizado la funciÃ³n especÃ­fica de ClickHouse toHour(), tal como pedÃ­a el enunciado.\n",
    "\n",
    "AnÃ¡lisis de la consulta:\n",
    "\n",
    "SELECT toHour(timestamp) AS hora_del_dia, count() ...: La funciÃ³n toHour(timestamp) es el corazÃ³n de la consulta. Extrae el componente \"hora\" (un nÃºmero de 0 a 23) de la columna timestamp. De esta forma, peticiones que ocurrieron a las 14:01:30, 14:25:10 y 14:55:00 son \"normalizadas\" y tratadas como parte del mismo grupo: la hora 14.\n",
    "\n",
    "GROUP BY hora_del_dia: Esta es la instrucciÃ³n de agregaciÃ³n. Le digo a ClickHouse que cree 24 \"cubos\" (uno para cada hora del dÃ­a) y que agrupe todas las filas de la tabla en el cubo que les corresponde segÃºn su hora_del_dia.\n",
    "\n",
    "count() AS total_peticiones: Una vez agrupadas las filas, count() cuenta cuÃ¡ntas peticiones cayeron en cada \"cubo\".\n",
    "\n",
    "ORDER BY total_peticiones DESC: Finalmente, ordenamos los resultados de mayor a menor para responder a la pregunta de las horas \"pico\" (peak), mostrando las franjas horarias con mÃ¡s actividad en primer lugar.\n",
    "\n",
    "Formateo en Python: La base de datos devuelve un nÃºmero (ej. 14). Para que el informe sea mÃ¡s legible, he formateado este resultado en Python para mostrarlo como una franja horaria clara: 14:00 - 15:00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado H*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â¿CuÃ¡les son los endpoints mÃ¡s solicitados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EJERCICIO 2 - Apartado H ---\n",
    "\n",
    "print(\"Identificando los endpoints mÃ¡s solicitados...\")\n",
    "\n",
    "try:\n",
    "    # 1. Defino la consulta\n",
    "    # Agrupamos por 'endpoint' y contamos las peticiones.\n",
    "    # Ordenamos DESC y usamos LIMIT 10 para ver el \"Top 10\".\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        endpoint,\n",
    "        count() AS total_solicitudes\n",
    "    FROM \n",
    "        web_access_logs \n",
    "    GROUP BY \n",
    "        endpoint\n",
    "    ORDER BY \n",
    "        total_solicitudes DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Ejecutamos la consulta\n",
    "    result = client.query(query)\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Top 10 endpoints mÃ¡s solicitados:\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    # 3. Itero y mostramos los resultados\n",
    "    if result.row_count > 0:\n",
    "        print(f\"{'Endpoint':<30} | {'Total Solicitudes':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        # row[0] es el endpoint (ej. '/usr/login'), row[1] es el total\n",
    "        for row in result.result_rows:\n",
    "            print(f\"{row[0]:<30} | {row[1]:<15}\")\n",
    "    else:\n",
    "        print(\"No se encontraron registros.\")\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al calcular los endpoints: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este anÃ¡lisis es muy similar al del Apartado G (horas pico), pero en lugar de agrupar por una dimensiÃ³n de tiempo, agrupamos por una dimensiÃ³n de recurso: el endpoint (la URL especÃ­fica que se solicitÃ³).\n",
    "\n",
    "AnÃ¡lisis de la consulta:\n",
    "\n",
    "SELECT endpoint, count() AS total_solicitudes: Seleccionamos la dimensiÃ³n endpoint y la mÃ©trica count(), que contarÃ¡ cuÃ¡ntas veces se ha solicitado cada endpoint Ãºnico.\n",
    "\n",
    "GROUP BY endpoint: Esta es la instrucciÃ³n de agregaciÃ³n. Le pido a ClickHouse que cree un \"montÃ³n\" para cada endpoint diferente (uno para /usr/login, uno para /usr/admin, etc.) y agrupe todas las filas de la tabla en el montÃ³n correspondiente.\n",
    "\n",
    "ORDER BY total_solicitudes DESC: Ordenamos los resultados de mayor a menor. Esto es crucial para responder a la pregunta de \"los mÃ¡s solicitados\", ya que pone las URLs mÃ¡s populares en la parte superior del informe.\n",
    "\n",
    "LIMIT 10: He aÃ±adido esta clÃ¡usula para acotar los resultados al \"Top 10\". En un sistema real con miles de endpoints diferentes, un LIMIT es fundamental para que el informe sea accionable y se centre en los recursos mÃ¡s relevantes (o con mÃ¡s carga) del servidor.\n",
    "\n",
    "El resultado nos permite identificar quÃ© partes de nuestra aplicaciÃ³n web son las mÃ¡s utilizadas por los usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado I*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula el tiempo promedio de respuesta por endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando el tiempo promedio de respuesta (ms) por endpoint...\n",
      "-------------------------------------------------\n",
      "Â¡Ã‰XITO! Top 10 endpoints mÃ¡s LENTOS (promedio):\n",
      "-------------------------------------------------\n",
      "Endpoint                       | Promedio (ms)  \n",
      "--------------------------------------------------\n",
      "/usr/login                     | 2666.93        \n",
      "/usr/register                  | 2616.23        \n",
      "/usr/admin                     | 2527.57        \n",
      "/usr/admin/developer           | 2492.04        \n",
      "/usr                           | 2346.59        \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado I ---\n",
    "\n",
    "print(\"Calculando el tiempo promedio de respuesta (ms) por endpoint...\")\n",
    "\n",
    "try:\n",
    "    # 1. Definimos la consulta\n",
    "    # Agrupamos por 'endpoint' y usamos AVG() sobre 'duration_ms'.\n",
    "    # Usamos round() para que el resultado sea legible (2 decimales).\n",
    "    # Ordenamos DESC para ver los endpoints MÃS LENTOS primero.\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        endpoint,\n",
    "        round(AVG(duration_ms), 2) AS avg_response_time_ms\n",
    "    FROM \n",
    "        web_access_logs \n",
    "    GROUP BY \n",
    "        endpoint\n",
    "    ORDER BY \n",
    "        avg_response_time_ms DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    # 2. Ejecutamos la consulta\n",
    "    result = client.query(query)\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Top 10 endpoints mÃ¡s LENTOS (promedio):\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    # 3. Iteramos y mostramos los resultados\n",
    "    if result.row_count > 0:\n",
    "        print(f\"{'Endpoint':<30} | {'Promedio (ms)':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        # row[0] es el endpoint, row[1] es el avg_response_time_ms (un float)\n",
    "        for row in result.result_rows:\n",
    "            # Usamos .2f para formatear el float con 2 decimales\n",
    "            print(f\"{row[0]:<30} | {row[1]:<15.2f}\")\n",
    "    else:\n",
    "        print(\"No se encontraron registros.\")\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al calcular el promedio: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este anÃ¡lisis es un paso mÃ¡s allÃ¡ del Apartado H. Ya no solo nos preguntamos \"quÃ© endpoints son los mÃ¡s populares\", sino \"Â¿cuÃ¡les son los que peor rendimiento tienen?\". Identificar esto es clave para encontrar cuellos de botella en la aplicaciÃ³n.\n",
    "\n",
    "AnÃ¡lisis de la consulta:\n",
    "\n",
    "SELECT endpoint, round(AVG(duration_ms), 2) AS ...:\n",
    "\n",
    "He seleccionado la dimensiÃ³n endpoint.\n",
    "\n",
    "He usado la funciÃ³n de agregaciÃ³n AVG(duration_ms). A diferencia de count(), AVG() (average) calcula la media aritmÃ©tica de todos los valores de duration_ms que pertenecen a ese grupo.\n",
    "\n",
    "He anidado el AVG() dentro de round(..., 2) para redondear el resultado a 2 decimales y obtener un informe limpio.\n",
    "\n",
    "GROUP BY endpoint: Al igual que antes, agrupo todas las filas que comparten el mismo endpoint para que la funciÃ³n AVG() pueda operar sobre cada grupo.\n",
    "\n",
    "ORDER BY avg_response_time_ms DESC: Esta es la parte mÃ¡s importante para el anÃ¡lisis. En lugar de ordenar por popularidad (conteo), he ordenado por el tiempo de respuesta promedio de forma descendente. Esto sitÃºa automÃ¡ticamente los endpoints mÃ¡s lentos (los mÃ¡s problemÃ¡ticos) en la parte superior del informe.\n",
    "\n",
    "LIMIT 10: Limito el resultado al \"Top 10\" de los peores endpoints.\n",
    "\n",
    "El resultado de esta consulta nos permite identificar quÃ© partes de la aplicaciÃ³n necesitan optimizaciÃ³n urgentemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado J*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtra las peticiones con un tiempo de respuesta superior a la media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrando peticiones con tiempo de respuesta superior a la media...\n",
      "La media global de respuesta es: 2533.40 ms\n",
      "Se encontraron 439 peticiones (del total de 871) por encima de la media.\n",
      "-------------------------------------------------\n",
      "Â¡Ã‰XITO! Top 10 peticiones mÃ¡s lentas (por encima de la media):\n",
      "-------------------------------------------------\n",
      "Endpoint                       | IP Address         | Respuesta (ms) \n",
      "-------------------------------------------------------------------\n",
      "/usr                           | 82.195.127.28      | 4999           \n",
      "/usr/login                     | 160.103.77.2       | 4989           \n",
      "/usr/login                     | 101.46.210.60      | 4987           \n",
      "/usr                           | 217.156.106.254    | 4979           \n",
      "/usr                           | 204.222.168.169    | 4973           \n",
      "/usr/login                     | 168.20.208.75      | 4973           \n",
      "/usr/admin/developer           | 70.218.223.153     | 4971           \n",
      "/usr                           | 65.68.196.59       | 4971           \n",
      "/usr/login                     | 173.212.181.209    | 4967           \n",
      "/usr/register                  | 57.139.252.41      | 4916           \n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado J ---\n",
    "\n",
    "print(\"Filtrando peticiones con tiempo de respuesta superior a la media...\")\n",
    "\n",
    "try:\n",
    "    # 1. Primero, necesitamos saber cuÃ¡l es la media global\n",
    "    avg_query = \"SELECT AVG(duration_ms) FROM web_access_logs\"\n",
    "    avg_result = client.query(avg_query)\n",
    "\n",
    "    # El resultado es [(2545.123...)]\n",
    "    media_global = avg_result.result_rows[0][0]\n",
    "\n",
    "    print(f\"La media global de respuesta es: {media_global:.2f} ms\")\n",
    "\n",
    "    # 2. Ahora, contamos cuÃ¡ntas peticiones estÃ¡n POR ENCIMA de esa media\n",
    "    # Usamos la variable 'media_global' que acabamos de calcular\n",
    "    count_query = f\"\"\"\n",
    "    SELECT \n",
    "        count()\n",
    "    FROM \n",
    "        web_access_logs \n",
    "    WHERE \n",
    "        duration_ms > {media_global}\n",
    "    \"\"\"\n",
    "    count_result = client.query(count_query)\n",
    "    total_lentas = count_result.result_rows[0][0]\n",
    "\n",
    "    print(f\"Se encontraron {total_lentas} peticiones (del total de 871) por encima de la media.\")\n",
    "\n",
    "    # 3. Finalmente, mostramos una muestra del \"Top 10\" de las mÃ¡s lentas\n",
    "    sample_query = f\"\"\"\n",
    "    SELECT \n",
    "        endpoint,\n",
    "        ip_address,\n",
    "        duration_ms\n",
    "    FROM \n",
    "        web_access_logs\n",
    "    WHERE\n",
    "        duration_ms > {media_global}\n",
    "    ORDER BY \n",
    "        duration_ms DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    sample_result = client.query(sample_query)\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Top 10 peticiones mÃ¡s lentas (por encima de la media):\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "    if sample_result.row_count > 0:\n",
    "        print(f\"{'Endpoint':<30} | {'IP Address':<18} | {'Respuesta (ms)':<15}\")\n",
    "        print(\"-\" * 67)\n",
    "        # row[0]=endpoint, row[1]=ip, row[2]=duration\n",
    "        for row in sample_result.result_rows:\n",
    "            print(f\"{row[0]:<30} | {row[1]:<18} | {row[2]:<15}\")\n",
    "    else:\n",
    "        print(\"No se encontraron registros.\")\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al filtrar por media: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este apartado requiere filtrar la tabla basÃ¡ndonos en un valor agregado, lo cual es una operaciÃ³n analÃ­tica clÃ¡sica. No podemos simplemente filtrar por un nÃºmero fijo (ej. > 3000), sino que debemos hacerlo con respecto a un valor dinÃ¡mico: la media de todos los datos.\n",
    "\n",
    "Mi implementaciÃ³n se ha dividido en tres fases para un anÃ¡lisis claro:\n",
    "\n",
    "CÃ¡lculo de la media (Baseline): Primero, he lanzado una consulta SELECT AVG(duration_ms) FROM ... para obtener un Ãºnico nÃºmero (media_global). Este nÃºmero es nuestra \"lÃ­nea de base\" o el umbral de rendimiento que separa una peticiÃ³n \"normal\" de una \"lenta\".\n",
    "\n",
    "Conteo de peticiones lentas: Con la media ya calculada en Python, he lanzado una segunda consulta SELECT count() ... WHERE duration_ms > {media_global}. Esto nos da una idea de la magnitud del problema: casi la mitad de las peticiones (434 de 871) son mÃ¡s lentas que la media, lo cual podrÃ­a sugerir una distribuciÃ³n de datos con una \"cola larga\" de peticiones muy lentas.\n",
    "\n",
    "Muestra de las peores peticiones: Simplemente filtrar no es suficiente para un buen anÃ¡lisis, ya que nos devolverÃ­a 434 filas. Para hacerlo accionable, he lanzado una tercera consulta que filtra (WHERE duration_ms > {media_global}), pero ademÃ¡s ordena (ORDER BY duration_ms DESC) y limita (LIMIT 10).\n",
    "\n",
    "Nota tÃ©cnica (Subqueries): La alternativa en SQL puro habrÃ­a sido usar una subquery en la clÃ¡usula WHERE (ej. ...WHERE duration_ms > (SELECT AVG(duration_ms) FROM ...)). He optado por calcular la media en un paso separado en Python (media_global) y luego inyectar ese valor en las consultas siguientes. Esto hace el cÃ³digo mÃ¡s legible y me da la media explÃ­citamente para poder imprimirla en el informe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Apartado K*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realiza una vista materializada (llamada â€œip_request_count_mvâ€) que nos permita llevar un registro de las direcciones IP y el nÃºmero de veces que aparecen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando la Vista Materializada 'ip_request_count_mv'...\n",
      "Vistas y tablas anteriores eliminadas (si existÃ­an).\n",
      "Tabla 'ip_request_count_table' (destino) creada.\n",
      "Vista Materializada 'ip_request_count_mv' creada.\n",
      "Poblando la vista con los datos existentes...\n",
      "---------------------------------\n",
      "Â¡Ã‰XITO! Vista Materializada creada y poblada.\n",
      "---------------------------------\n",
      "Prueba: Consultando la tabla 'ip_request_count_table' (Top 5 IPs):\n",
      "IP Address         | Total Peticiones\n",
      "-------------------------------------\n",
      "204.222.168.169    | 50             \n",
      "32.180.237.129     | 1              \n",
      "33.32.243.166      | 1              \n",
      "32.31.12.193       | 1              \n",
      "32.224.240.39      | 1              \n"
     ]
    }
   ],
   "source": [
    "# --- EJERCICIO 2 - Apartado K ---\n",
    "\n",
    "print(\"Creando la Vista Materializada 'ip_request_count_mv'...\")\n",
    "\n",
    "try:\n",
    "    # 1. Limpiamos por si existÃ­an de una prueba anterior\n",
    "    client.command(\"DROP VIEW IF EXISTS ip_request_count_mv\")\n",
    "    client.command(\"DROP TABLE IF EXISTS ip_request_count_table\")\n",
    "    print(\"Vistas y tablas anteriores eliminadas (si existÃ­an).\")\n",
    "\n",
    "    # 2. Creamos la tabla destino (la tabla \"resumen\")\n",
    "    # Esta tabla usarÃ¡ un motor especial para agregar datos\n",
    "    client.command(\"\"\"\n",
    "    CREATE TABLE ip_request_count_table (\n",
    "        ip_address String,\n",
    "        total_requests UInt64\n",
    "    ) ENGINE = SummingMergeTree()\n",
    "    ORDER BY ip_address\n",
    "    \"\"\")\n",
    "    print(\"Tabla 'ip_request_count_table' (destino) creada.\")\n",
    "\n",
    "    # 3. Creamos la Vista Materializada\n",
    "    # Esta vista se \"engancha\" a 'web_access_logs'\n",
    "    # Cada vez que algo se INSERTA en 'web_access_logs',\n",
    "    # esta vista coge el dato, lo procesa, y lo inserta en 'ip_request_count_table'.\n",
    "    client.command(\"\"\"\n",
    "    CREATE MATERIALIZED VIEW ip_request_count_mv\n",
    "    TO ip_request_count_table\n",
    "    AS \n",
    "    SELECT \n",
    "        ip_address,\n",
    "        count() AS total_requests\n",
    "    FROM \n",
    "        web_access_logs\n",
    "    GROUP BY \n",
    "        ip_address\n",
    "    \"\"\")\n",
    "    print(\"Vista Materializada 'ip_request_count_mv' creada.\")\n",
    "\n",
    "    # 4. (IMPORTANTE) La vista solo funciona para DATOS NUEVOS.\n",
    "    # Tenemos que \"rellenarla\" con los 871 datos que ya tenÃ­amos.\n",
    "    print(\"Poblando la vista con los datos existentes...\")\n",
    "    client.command(\"\"\"\n",
    "    INSERT INTO ip_request_count_table\n",
    "    SELECT \n",
    "        ip_address,\n",
    "        count() AS total_requests\n",
    "    FROM \n",
    "        web_access_logs\n",
    "    GROUP BY \n",
    "        ip_address\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Â¡Ã‰XITO! Vista Materializada creada y poblada.\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "    # 5. Hacemos una prueba para ver si funciona\n",
    "    print(\"Prueba: Consultando la tabla 'ip_request_count_table' (Top 5 IPs):\")\n",
    "    result = client.query(\"SELECT * FROM ip_request_count_table ORDER BY total_requests DESC LIMIT 5\")\n",
    "\n",
    "    print(f\"{'IP Address':<18} | {'Total Peticiones':<15}\")\n",
    "    print(\"-\" * 37)\n",
    "    for row in result.result_rows:\n",
    "        print(f\"{row[0]:<18} | {row[1]:<15}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"â›” ERROR al crear la Vista Materializada: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplicaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el concepto mÃ¡s avanzado de la prÃ¡ctica. Una Vista Materializada (MV) en ClickHouse es un trigger (un disparador) que transforma y almacena datos de forma proactiva.\n",
    "\n",
    "Diferencia clave con una VIEW normal:\n",
    "\n",
    "Una VIEW normal es solo un \"alias\" o una consulta guardada. Cada vez que la consultas, la base de datos tiene que hacer todo el trabajo (escanear la tabla web_access_logs y contar 871 filas) en ese momento.\n",
    "\n",
    "Una MATERIALIZED VIEW hace el trabajo por adelantado. Mantiene una tabla separada con los resultados ya calculados. Cuando la consultas, solo tiene que leer esa tabla-resumen (que es mucho mÃ¡s pequeÃ±a y rÃ¡pida).\n",
    "\n",
    "Mi implementaciÃ³n en detalle:\n",
    "\n",
    "DROP...: Primero, me aseguro de limpiar cualquier intento anterior para que el script sea re-ejecutable.\n",
    "\n",
    "CREATE TABLE ip_request_count_table ... ENGINE = SummingMergeTree(): He creado una tabla \"destino\" para la MV. El motor SummingMergeTree() es clave: estÃ¡ optimizado para agregar datos. Si insertamos ('1.1.1.1', 5) y luego ('1.1.1.1', 10), ClickHouse los \"aplastarÃ¡\" automÃ¡ticamente en una sola fila ('1.1.1.1', 15).\n",
    "\n",
    "CREATE MATERIALIZED VIEW ... TO ip_request_count_table ... AS SELECT ...: Esta es la creaciÃ³n de la MV.\n",
    "\n",
    "TO ip_request_count_table: Le dice a la MV dÃ³nde debe guardar los resultados.\n",
    "\n",
    "AS SELECT ...: Esta es la \"receta\" de transformaciÃ³n. Le dice a la MV: \"Cuando veas un nuevo lote de datos (INSERT) en web_access_logs, ejecuta esta consulta GROUP BY sobre ese lote, y guarda el resultado en ip_request_count_table\".\n",
    "\n",
    "INSERT INTO ip_request_count_table ...: Este es el paso crucial. La MV solo funciona para datos nuevos que se inserten despuÃ©s de su creaciÃ³n. Los 871 registros que ya estaban en web_access_logs no la activan. Por lo tanto, he ejecutado la consulta SELECT ... GROUP BY manualmente una vez para \"poblar\" o \"rellenar\" la tabla ip_request_count_table con los datos histÃ³ricos.\n",
    "\n",
    "A partir de este momento, si hiciÃ©ramos nuevas inserciones (como en la simulaciÃ³n en tiempo real), la tabla ip_request_count_table se actualizarÃ­a sola, y consultarla serÃ­a instantÃ¡neo, sin importar si web_access_logs crece a 800 mil o 800 millones de filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando la Vista Materializada vs. la Tabla Original...\n",
      "(Si los nÃºmeros coinciden, la MV funciona perfectamente)\n",
      "\n",
      "---------------------------------\n",
      "Â¡VERIFICACIÃ“N CORRECTA! âœ…\n",
      "La tabla resumen 'ip_request_count_table' tiene 822 IPs Ãºnicas.\n",
      "Los conteos coinciden 1 a 1 con un recÃ¡lculo manual de la tabla 'web_access_logs'.\n",
      "La Vista Materializada funciona.\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- VERIFICACIÃ“N FINAL ---\n",
    "\n",
    "print(\"Verificando la Vista Materializada vs. la Tabla Original...\")\n",
    "print(\"(Si los nÃºmeros coinciden, la MV funciona perfectamente)\")\n",
    "\n",
    "# 1. Obtenemos los datos de la VISTA (rÃ¡pido)\n",
    "query_vista = \"SELECT ip_address, total_requests FROM ip_request_count_table ORDER BY ip_address\"\n",
    "vista_result = client.query(query_vista).result_rows\n",
    "\n",
    "# 2. Obtenemos los datos de la TABLA ORIGINAL (lento)\n",
    "query_original = \"\"\"\n",
    "SELECT \n",
    "    ip_address,\n",
    "    count() AS total_requests\n",
    "FROM \n",
    "    web_access_logs\n",
    "GROUP BY \n",
    "    ip_address\n",
    "ORDER BY\n",
    "    ip_address\n",
    "\"\"\"\n",
    "original_result = client.query(query_original).result_rows\n",
    "\n",
    "# 3. Comparamos\n",
    "if vista_result == original_result:\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(\"Â¡VERIFICACIÃ“N CORRECTA! âœ…\")\n",
    "    print(f\"La tabla resumen 'ip_request_count_table' tiene {len(vista_result)} IPs Ãºnicas.\")\n",
    "    print(\"Los conteos coinciden 1 a 1 con un recÃ¡lculo manual de la tabla 'web_access_logs'.\")\n",
    "    print(\"La Vista Materializada funciona.\")\n",
    "    print(\"---------------------------------\")\n",
    "else:\n",
    "    print(\"\\nâ›” Â¡ERROR DE VERIFICACIÃ“N! Los datos no coinciden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clickhouse-env)",
   "language": "python",
   "name": "clickhouse-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e91f8003283cb1c30f8e99a064808c421c5f75be26e4d17ac3428a192e00edd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
